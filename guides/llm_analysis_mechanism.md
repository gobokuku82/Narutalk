# LLM ë¶„ì„ ë° ê³„íš ìˆ˜ë¦½ ë©”ì»¤ë‹ˆì¦˜ ìƒì„¸ ê°€ì´ë“œ

## ğŸ¯ ê°œìš”

Supervisor Agentì˜ í•µì‹¬ì€ **GPT-4oë¥¼ í™œìš©í•œ ì§€ëŠ¥í˜• ì§ˆì˜ ë¶„ì„ê³¼ ë™ì  ê³„íš ìˆ˜ë¦½**ì…ë‹ˆë‹¤. ì´ ë¬¸ì„œëŠ” LLMì´ ì–´ë–»ê²Œ ì‚¬ìš©ìì˜ ë³µì¡í•œ ì§ˆì˜ë¥¼ ì´í•´í•˜ê³ , ìµœì ì˜ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•˜ëŠ”ì§€ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.

---

## ğŸ“Š LLM ë¶„ì„ í”„ë¡œì„¸ìŠ¤

### 1. ì§ˆì˜ ë¶„ì„ íŒŒì´í”„ë¼ì¸

```mermaid
graph LR
    A[ì‚¬ìš©ì ì§ˆì˜] --> B[ì „ì²˜ë¦¬]
    B --> C[LLM í”„ë¡¬í”„íŒ…]
    C --> D[JSON íŒŒì‹±]
    D --> E[ê²€ì¦ ë° ë³´ì •]
    E --> F[ë¶„ì„ ê²°ê³¼]
    
    B --> B1[ë…¸ì´ì¦ˆ ì œê±°]
    B --> B2[ì•½ì–´ í™•ì¥]
    
    C --> C1[System Prompt]
    C --> C2[Few-shot Examples]
    C --> C3[User Query]
    
    E --> E1[ìŠ¤í‚¤ë§ˆ ê²€ì¦]
    E --> E2[ê¸°ë³¸ê°’ ì±„ìš°ê¸°]
```

### 2. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì „ëµ

#### 2.1 êµ¬ì¡°í™”ëœ System Prompt

```python
ANALYSIS_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì œì•½íšŒì‚¬ ì „ë¬¸ AI ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ì ì§ˆë¬¸ì„ ë‹¤ìŒ í”„ë ˆì„ì›Œí¬ë¡œ ë¶„ì„í•˜ì„¸ìš”:

## ë¶„ì„ í”„ë ˆì„ì›Œí¬

### 1. ì˜ë„ ë¶„ë¥˜ (Intent Classification)
- **analysis**: ë°ì´í„° ë¶„ì„, í†µê³„, íŠ¸ë Œë“œ, ì‹¤ì  ê´€ë ¨
- **search**: ì •ë³´ ê²€ìƒ‰, ìë£Œ ì°¾ê¸°, ë°ì´í„° ì¡°íšŒ
- **document**: ë³´ê³ ì„œ ì‘ì„±, ë¬¸ì„œ ìƒì„±, í…œí”Œë¦¿ í™œìš©
- **customer**: ê³ ê°/ê±°ë˜ì²˜ ë¶„ì„, ê´€ê³„ ê´€ë¦¬

### 2. ë³µì¡ë„ í‰ê°€ (Complexity Assessment)
```
0.0-0.3: ë‹¨ìˆœ ì¡°íšŒ (ì˜ˆ: "Aë³‘ì› ì—°ë½ì²˜ëŠ”?")
0.3-0.5: ê¸°ë³¸ ë¶„ì„ (ì˜ˆ: "ì´ë²ˆë‹¬ ë§¤ì¶œì€?")
0.5-0.7: ë³µí•© ë¶„ì„ (ì˜ˆ: "ì „ë…„ ëŒ€ë¹„ ì„±ì¥ë¥  ë¶„ì„")
0.7-0.9: ì‹¬í™” ë¶„ì„ (ì˜ˆ: "ì§€ì—­ë³„ ì œí’ˆë³„ ì‹¤ì  ë¹„êµ ë¶„ì„")
0.9-1.0: í†µí•© ë¶„ì„ (ì˜ˆ: "ì „ì‚¬ í†µí•© ë¦¬í¬íŠ¸ ìƒì„±")
```

### 3. ì—”í‹°í‹° ì¶”ì¶œ (Entity Extraction)
- **íšŒì‚¬/ê±°ë˜ì²˜**: ë³‘ì›, ì•½êµ­, ì œì•½íšŒì‚¬ëª…
- **ì œí’ˆ**: ì˜ì•½í’ˆëª…, ì œí’ˆ ì¹´í…Œê³ ë¦¬
- **ì‹œê°„**: ê¸°ê°„, ë‚ ì§œ, ë¶„ê¸°, ë…„ë„
- **ì§€ì—­**: ë„ì‹œ, ì§€ì—­, ì˜ì—…ì†Œ
- **ì§€í‘œ**: ë§¤ì¶œ, ìˆ˜ëŸ‰, ì ìœ ìœ¨ ë“±

### 4. í•„ìš” ì—ì´ì „íŠ¸ ë§¤í•‘
ì§ˆì˜ íŒ¨í„´ë³„ ì—ì´ì „íŠ¸ ì¡°í•©:
- ë‹¨ìˆœ ì¡°íšŒ â†’ [search]
- ì‹¤ì  ë¶„ì„ â†’ [analysis, search]
- ë³´ê³ ì„œ ì‘ì„± â†’ [search, analysis, document]
- ê³ ê° ì „ëµ â†’ [customer, analysis, search]

## ì¶œë ¥ í˜•ì‹
ë°˜ë“œì‹œ ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¥´ì„¸ìš”:
{
    "intent": "primary_intent",
    "required_agents": ["agent1", "agent2"],
    "entities": [
        {"type": "entity_type", "value": "entity_value", "confidence": 0.95}
    ],
    "complexity": 0.0-1.0,
    "keywords": ["key1", "key2"],
    "context_needed": true/false,
    "suggested_followup": ["possible_question1"]
}
"""
```

#### 2.2 Few-shot Learning Examples

```python
FEW_SHOT_EXAMPLES = [
    {
        "query": "í•œêµ­ì œì•½ 3ë¶„ê¸° íƒ€ì´ë ˆë†€ ë§¤ì¶œ ì‹¤ì  ë¶„ì„í•´ì¤˜",
        "analysis": {
            "intent": "analysis",
            "required_agents": ["analysis", "search"],
            "entities": [
                {"type": "company", "value": "í•œêµ­ì œì•½", "confidence": 1.0},
                {"type": "period", "value": "3ë¶„ê¸°", "confidence": 1.0},
                {"type": "product", "value": "íƒ€ì´ë ˆë†€", "confidence": 1.0},
                {"type": "metric", "value": "ë§¤ì¶œ", "confidence": 0.95}
            ],
            "complexity": 0.6,
            "keywords": ["ë§¤ì¶œ", "ì‹¤ì ", "ë¶„ì„", "3ë¶„ê¸°"],
            "context_needed": false,
            "suggested_followup": [
                "ì „ë…„ ë™ê¸° ëŒ€ë¹„ ì„±ì¥ë¥ ì€?",
                "ê²½ìŸì‚¬ ëŒ€ë¹„ ì‹¤ì ì€?"
            ]
        }
    },
    {
        "query": "ì„œìš¸ì§€ì—­ ì£¼ìš” ë³‘ì› ë¦¬ìŠ¤íŠ¸ì™€ ë‹´ë‹¹ì ì—°ë½ì²˜ ì°¾ì•„ì„œ ë°©ë¬¸ ê³„íšì„œ ë§Œë“¤ì–´ì¤˜",
        "analysis": {
            "intent": "document",
            "required_agents": ["search", "customer", "document"],
            "entities": [
                {"type": "region", "value": "ì„œìš¸ì§€ì—­", "confidence": 1.0},
                {"type": "customer_type", "value": "ë³‘ì›", "confidence": 1.0},
                {"type": "document_type", "value": "ë°©ë¬¸ ê³„íšì„œ", "confidence": 0.9}
            ],
            "complexity": 0.75,
            "keywords": ["ë³‘ì›", "ë¦¬ìŠ¤íŠ¸", "ë‹´ë‹¹ì", "ë°©ë¬¸", "ê³„íšì„œ"],
            "context_needed": true,
            "suggested_followup": [
                "ë°©ë¬¸ ìš°ì„ ìˆœìœ„ ì„¤ì • ê¸°ì¤€ì€?",
                "ì´ì „ ë°©ë¬¸ ì´ë ¥ í™•ì¸"
            ]
        }
    }
]
```

### 3. LLM í˜¸ì¶œ ìµœì í™”

#### 3.1 Temperature ì¡°ì • ì „ëµ

```python
def get_optimal_temperature(query_type):
    """ì§ˆì˜ íƒ€ì…ë³„ ìµœì  temperature ì„¤ì •"""
    
    TEMPERATURE_MAP = {
        "factual_lookup": 0.1,      # ì‚¬ì‹¤ ì¡°íšŒ: ì¼ê´€ì„± ì¤‘ìš”
        "analysis": 0.3,            # ë¶„ì„: ì•½ê°„ì˜ ì°½ì˜ì„±
        "creative_document": 0.7,   # ë¬¸ì„œ ìƒì„±: ì°½ì˜ì„± í•„ìš”
        "brainstorming": 0.9        # ì•„ì´ë””ì–´: ë†’ì€ ì°½ì˜ì„±
    }
    
    # ì§ˆì˜ íŠ¹ì„± ë¶„ì„
    if "ì •í™•í•œ" in query or "êµ¬ì²´ì " in query:
        return 0.1
    elif "ë¶„ì„" in query or "ë¹„êµ" in query:
        return 0.3
    elif "ì œì•ˆ" in query or "ì¶”ì²œ" in query:
        return 0.5
    else:
        return 0.7
```

#### 3.2 í† í° ìµœì í™”

```python
class TokenOptimizer:
    def optimize_prompt(self, base_prompt, max_tokens=2000):
        """í”„ë¡¬í”„íŠ¸ í† í° ìµœì í™”"""
        
        # í† í° ì¹´ìš´íŠ¸
        current_tokens = self.count_tokens(base_prompt)
        
        if current_tokens > max_tokens:
            # ìš°ì„ ìˆœìœ„ë³„ ì••ì¶•
            optimized = self.compress_prompt(base_prompt, {
                "examples": 0.3,      # 30% ìœ ì§€
                "instructions": 0.9,  # 90% ìœ ì§€
                "context": 0.5       # 50% ìœ ì§€
            })
            return optimized
        
        return base_prompt
    
    def compress_prompt(self, prompt, retention_rates):
        """ì„ íƒì  í”„ë¡¬í”„íŠ¸ ì••ì¶•"""
        sections = self.parse_sections(prompt)
        compressed = {}
        
        for section, content in sections.items():
            rate = retention_rates.get(section, 1.0)
            if rate < 1.0:
                # ì¤‘ìš”ë„ ê¸°ë°˜ ë¬¸ì¥ ì„ íƒ
                compressed[section] = self.select_important(content, rate)
            else:
                compressed[section] = content
        
        return self.reconstruct_prompt(compressed)
```

---

## ğŸ—ºï¸ ë™ì  ê³„íš ìˆ˜ë¦½ ë©”ì»¤ë‹ˆì¦˜

### 1. ê³„íš ìˆ˜ë¦½ ì•Œê³ ë¦¬ì¦˜

#### 1.1 ì˜ì¡´ì„± ê·¸ë˜í”„ êµ¬ì¶•

```python
class DependencyGraphBuilder:
    """ì—ì´ì „íŠ¸ ê°„ ì˜ì¡´ì„± ê·¸ë˜í”„ êµ¬ì¶•"""
    
    # ì—ì´ì „íŠ¸ ì˜ì¡´ì„± ì •ì˜
    AGENT_DEPENDENCIES = {
        "analysis": {
            "requires": [],  # ë…ë¦½ì 
            "provides": ["statistics", "insights", "trends"]
        },
        "search": {
            "requires": [],  # ë…ë¦½ì 
            "provides": ["raw_data", "documents", "information"]
        },
        "document": {
            "requires": ["raw_data", "statistics"],  # search, analysis í•„ìš”
            "provides": ["reports", "presentations"]
        },
        "customer": {
            "requires": ["raw_data"],  # search í•„ìš”
            "provides": ["customer_insights", "recommendations"]
        }
    }
    
    def build_graph(self, required_agents):
        """ì˜ì¡´ì„± ê·¸ë˜í”„ ìƒì„±"""
        graph = {}
        
        for agent in required_agents:
            dependencies = []
            agent_requires = self.AGENT_DEPENDENCIES[agent]["requires"]
            
            for other_agent in required_agents:
                if agent != other_agent:
                    other_provides = self.AGENT_DEPENDENCIES[other_agent]["provides"]
                    
                    # ì˜ì¡´ì„± ì²´í¬
                    if any(req in other_provides for req in agent_requires):
                        dependencies.append(other_agent)
            
            graph[agent] = dependencies
        
        return graph
    
    def topological_sort(self, graph):
        """í† í´ë¡œì§€ ì •ë ¬ë¡œ ì‹¤í–‰ ìˆœì„œ ê²°ì •"""
        in_degree = {node: 0 for node in graph}
        
        for node in graph:
            for dep in graph[node]:
                in_degree[dep] += 1
        
        queue = [node for node in graph if in_degree[node] == 0]
        result = []
        
        while queue:
            # ê°™ì€ ë ˆë²¨ ë…¸ë“œë“¤ (ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥)
            level = []
            next_queue = []
            
            for node in queue:
                level.append(node)
                for neighbor in graph[node]:
                    in_degree[neighbor] -= 1
                    if in_degree[neighbor] == 0:
                        next_queue.append(neighbor)
            
            result.append(level)
            queue = next_queue
        
        return result
```

#### 1.2 ì‹¤í–‰ ê³„íš ìƒì„±

```python
class ExecutionPlanner:
    def create_execution_plan(self, query_analysis):
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤í–‰ ê³„íš ìƒì„±"""
        
        plan = {
            "version": "1.0",
            "created_at": datetime.now().isoformat(),
            "query_complexity": query_analysis["complexity"],
            "steps": []
        }
        
        # 1. ì˜ì¡´ì„± ê·¸ë˜í”„ êµ¬ì¶•
        dep_builder = DependencyGraphBuilder()
        dep_graph = dep_builder.build_graph(query_analysis["required_agents"])
        
        # 2. ì‹¤í–‰ ìˆœì„œ ê²°ì • (í† í´ë¡œì§€ ì •ë ¬)
        execution_levels = dep_builder.topological_sort(dep_graph)
        
        # 3. ê° ë ˆë²¨ë³„ ì‹¤í–‰ ê³„íš ìƒì„±
        step_id = 1
        for level_idx, agents_in_level in enumerate(execution_levels):
            
            # ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥ ì—¬ë¶€
            can_parallel = len(agents_in_level) > 1
            
            for agent in agents_in_level:
                step = {
                    "step_id": f"step_{step_id}",
                    "agent_name": agent,
                    "execution_level": level_idx,
                    "parallel": can_parallel,
                    "dependencies": dep_graph.get(agent, []),
                    "estimated_time": self.estimate_time(agent, query_analysis),
                    "priority": self.calculate_priority(agent, query_analysis),
                    "retry_policy": self.get_retry_policy(agent),
                    "timeout": self.calculate_timeout(agent, query_analysis)
                }
                
                plan["steps"].append(step)
                step_id += 1
        
        # 4. ìµœì í™” ì ìš©
        plan = self.optimize_plan(plan, query_analysis)
        
        return plan
    
    def estimate_time(self, agent, analysis):
        """ì—ì´ì „íŠ¸ë³„ ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°"""
        
        BASE_TIMES = {
            "analysis": 5,
            "search": 3,
            "document": 8,
            "customer": 4
        }
        
        base = BASE_TIMES.get(agent, 5)
        
        # ë³µì¡ë„ ê°€ì¤‘ì¹˜
        complexity_factor = 1 + analysis["complexity"]
        
        # ì—”í‹°í‹° ìˆ˜ ê°€ì¤‘ì¹˜
        entity_factor = 1 + (len(analysis["entities"]) * 0.1)
        
        return base * complexity_factor * entity_factor
    
    def calculate_priority(self, agent, analysis):
        """ì—ì´ì „íŠ¸ ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        
        PRIORITY_WEIGHTS = {
            "analysis": 0.9,   # ë†’ì€ ìš°ì„ ìˆœìœ„
            "search": 0.8,
            "customer": 0.6,
            "document": 0.5    # ë‚®ì€ ìš°ì„ ìˆœìœ„ (ì˜ì¡´ì )
        }
        
        base_priority = PRIORITY_WEIGHTS.get(agent, 0.5)
        
        # ì‚¬ìš©ì ì˜ë„ì™€ ì¼ì¹˜ë„
        intent_match = 1.0 if analysis["intent"] == agent else 0.7
        
        return base_priority * intent_match
```

### 2. ê³„íš ìµœì í™” ì „ëµ

#### 2.1 ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”

```python
class ParallelOptimizer:
    def optimize_for_parallel(self, plan):
        """ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”"""
        
        optimized_steps = []
        
        for level in self.group_by_level(plan["steps"]):
            # ë¦¬ì†ŒìŠ¤ ì œì•½ í™•ì¸
            if self.can_run_parallel(level):
                # ë³‘ë ¬ ì‹¤í–‰ ë§ˆí‚¹
                for step in level:
                    step["execution_mode"] = "parallel"
                    step["thread_pool"] = "shared"
            else:
                # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ìˆœì°¨ ì‹¤í–‰
                sorted_level = sorted(level, 
                                    key=lambda x: x["priority"], 
                                    reverse=True)
                for idx, step in enumerate(sorted_level):
                    step["execution_mode"] = "sequential"
                    step["execution_order"] = idx
            
            optimized_steps.extend(level)
        
        plan["steps"] = optimized_steps
        return plan
    
    def can_run_parallel(self, steps):
        """ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨"""
        
        # ë¦¬ì†ŒìŠ¤ ê³„ì‚°
        total_memory = sum(self.estimate_memory(s) for s in steps)
        total_cpu = sum(self.estimate_cpu(s) for s in steps)
        
        # ì„ê³„ê°’ ì²´í¬
        return (total_memory < MAX_MEMORY_THRESHOLD and 
                total_cpu < MAX_CPU_THRESHOLD and
                len(steps) <= MAX_PARALLEL_AGENTS)
```

#### 2.2 ìºì‹± ì „ëµ

```python
class CachingStrategy:
    def apply_caching(self, plan, historical_data):
        """ìºì‹± ì „ëµ ì ìš©"""
        
        for step in plan["steps"]:
            agent = step["agent_name"]
            
            # ìºì‹œ ê°€ëŠ¥ì„± í‰ê°€
            cache_score = self.evaluate_cachability(step, historical_data)
            
            if cache_score > 0.7:
                step["cache_strategy"] = "aggressive"
                step["cache_ttl"] = 3600  # 1ì‹œê°„
            elif cache_score > 0.4:
                step["cache_strategy"] = "moderate"
                step["cache_ttl"] = 900   # 15ë¶„
            else:
                step["cache_strategy"] = "minimal"
                step["cache_ttl"] = 300    # 5ë¶„
            
            # ìºì‹œ í‚¤ ìƒì„± ì „ëµ
            step["cache_key_pattern"] = self.generate_cache_key_pattern(step)
        
        return plan
    
    def evaluate_cachability(self, step, historical_data):
        """ìºì‹œ ê°€ëŠ¥ì„± ì ìˆ˜ ê³„ì‚°"""
        
        score = 0.0
        
        # 1. ì—ì´ì „íŠ¸ íƒ€ì…ë³„ ê¸°ë³¸ ì ìˆ˜
        cachable_agents = {"search": 0.8, "analysis": 0.6}
        score += cachable_agents.get(step["agent_name"], 0.3)
        
        # 2. íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ì¬ì‚¬ìš©ë¥ 
        reuse_rate = historical_data.get_reuse_rate(step["agent_name"])
        score += reuse_rate * 0.3
        
        # 3. ë°ì´í„° ë³€ë™ì„±
        volatility = historical_data.get_data_volatility(step["agent_name"])
        score -= volatility * 0.2
        
        return min(max(score, 0.0), 1.0)
```

### 3. ì¸í„°ëŸ½íŠ¸ í¬ì¸íŠ¸ ì„¤ì •

#### 3.1 ë™ì  ì¸í„°ëŸ½íŠ¸ ê²°ì •

```python
class InterruptManager:
    def set_interrupt_points(self, plan, context):
        """ë™ì  ì¸í„°ëŸ½íŠ¸ í¬ì¸íŠ¸ ì„¤ì •"""
        
        for step in plan["steps"]:
            # ê¸°ë³¸ ì¸í„°ëŸ½íŠ¸ í•„ìš” ì—¬ë¶€
            requires_interrupt = self.check_base_requirements(step, context)
            
            # ë™ì  ì¡°ê±´ í‰ê°€
            if not requires_interrupt:
                requires_interrupt = self.evaluate_dynamic_conditions(step, context)
            
            if requires_interrupt:
                step["interrupt"] = {
                    "enabled": True,
                    "type": self.determine_interrupt_type(step),
                    "message": self.generate_interrupt_message(step),
                    "options": self.get_interrupt_options(step),
                    "timeout": 60,  # 60ì´ˆ ëŒ€ê¸°
                    "default_action": "proceed"  # íƒ€ì„ì•„ì›ƒ ì‹œ ê¸°ë³¸ ë™ì‘
                }
            else:
                step["interrupt"] = {"enabled": False}
        
        return plan
    
    def check_base_requirements(self, step, context):
        """ê¸°ë³¸ ì¸í„°ëŸ½íŠ¸ ìš”êµ¬ì‚¬í•­ ì²´í¬"""
        
        # ì»¨í…ìŠ¤íŠ¸ ëª¨ë“œ í™•ì¸
        if context.interrupt_mode == "none":
            return False
        if context.interrupt_mode == "all":
            return True
        
        # Critical ëª¨ë“œ: íŠ¹ì • ì‘ì—…ë§Œ
        critical_actions = [
            "sql_execution",
            "data_modification",
            "external_api_call",
            "document_generation"
        ]
        
        return any(action in step.get("action", "").lower() 
                  for action in critical_actions)
    
    def evaluate_dynamic_conditions(self, step, context):
        """ë™ì  ì¡°ê±´ í‰ê°€"""
        
        # ë¹„ìš© ì„ê³„ê°’ ì²´í¬
        if step.get("estimated_cost", 0) > context.cost_threshold:
            return True
        
        # ë¯¼ê° ë°ì´í„° ì ‘ê·¼
        if step.get("accesses_sensitive_data", False):
            return True
        
        # ì‚¬ìš©ì ì •ì˜ ê·œì¹™
        for rule in context.custom_rules:
            if rule.matches(step):
                return True
        
        return False
```

---

## ğŸš€ ê³ ê¸‰ ìµœì í™” ê¸°ë²•

### 1. ì ì‘í˜• í•™ìŠµ ì‹œìŠ¤í…œ

```python
class AdaptiveLearningSystem:
    def __init__(self):
        self.performance_history = []
        self.pattern_database = {}
        
    def learn_from_execution(self, query, plan, result):
        """ì‹¤í–‰ ê²°ê³¼ë¡œë¶€í„° í•™ìŠµ"""
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        metrics = {
            "query_pattern": self.extract_pattern(query),
            "plan_efficiency": self.calculate_efficiency(plan, result),
            "user_satisfaction": result.get("satisfaction", 0.5),
            "execution_time": result["total_time"],
            "resource_usage": result["resources"]
        }
        
        # íŒ¨í„´ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
        pattern = metrics["query_pattern"]
        if pattern not in self.pattern_database:
            self.pattern_database[pattern] = []
        
        self.pattern_database[pattern].append({
            "plan": plan,
            "metrics": metrics
        })
        
        # ìµœì  ì „ëµ ì¬ê³„ì‚°
        self.recalculate_optimal_strategies()
    
    def suggest_plan_improvements(self, current_plan):
        """í˜„ì¬ ê³„íš ê°œì„  ì œì•ˆ"""
        
        improvements = []
        
        # ìœ ì‚¬ íŒ¨í„´ì˜ ì„±ê³µ ì‚¬ë¡€ ì°¾ê¸°
        similar_successful = self.find_similar_successful_plans(current_plan)
        
        for success_case in similar_successful:
            diff = self.calculate_plan_diff(current_plan, success_case["plan"])
            
            if diff["improvement_potential"] > 0.2:
                improvements.append({
                    "suggestion": diff["changes"],
                    "expected_improvement": diff["improvement_potential"],
                    "confidence": diff["confidence"]
                })
        
        return sorted(improvements, 
                     key=lambda x: x["expected_improvement"], 
                     reverse=True)
```

### 2. ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ í–¥ìƒ

```python
class ContextAwareAnalyzer:
    def enhance_with_context(self, query, session_history):
        """ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ì¿¼ë¦¬ ê°•í™”"""
        
        enhanced_query = {
            "original": query,
            "context_entities": [],
            "implicit_requirements": [],
            "conversation_flow": []
        }
        
        # 1. ëŒ€ëª…ì‚¬ í•´ê²°
        enhanced_query["resolved"] = self.resolve_references(
            query, 
            session_history
        )
        
        # 2. ì•”ë¬µì  ìš”êµ¬ì‚¬í•­ ì¶”ë¡ 
        enhanced_query["implicit_requirements"] = self.infer_requirements(
            query,
            session_history
        )
        
        # 3. ëŒ€í™” íë¦„ ë¶„ì„
        enhanced_query["conversation_flow"] = self.analyze_flow(
            session_history
        )
        
        # 4. ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
        enhanced_query["context_entities"] = self.extract_context_entities(
            session_history[-5:]  # ìµœê·¼ 5ê°œ ëŒ€í™”
        )
        
        return enhanced_query
    
    def resolve_references(self, query, history):
        """ëŒ€ëª…ì‚¬ ë° ì°¸ì¡° í•´ê²°"""
        
        # ëŒ€ëª…ì‚¬ ë§¤í•‘
        pronouns = {
            "ê·¸ê²ƒ": self.find_last_mentioned_entity(history, "object"),
            "ê·¸ê³³": self.find_last_mentioned_entity(history, "location"),
            "ê·¸ë•Œ": self.find_last_mentioned_entity(history, "time"),
            "ê°™ì€": self.find_last_mentioned_entity(history, "any")
        }
        
        resolved = query
        for pronoun, entity in pronouns.items():
            if pronoun in query and entity:
                resolved = resolved.replace(pronoun, entity)
        
        return resolved
```

### 3. í•˜ì´ë¸Œë¦¬ë“œ ê³„íš ì „ëµ

```python
class HybridPlanningStrategy:
    def create_hybrid_plan(self, query_analysis):
        """ê·œì¹™ ê¸°ë°˜ + ML ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê³„íš"""
        
        # 1. ê·œì¹™ ê¸°ë°˜ ì´ˆê¸° ê³„íš
        rule_based_plan = self.create_rule_based_plan(query_analysis)
        
        # 2. ML ëª¨ë¸ ì˜ˆì¸¡
        ml_predicted_plan = self.ml_model.predict_plan(query_analysis)
        
        # 3. ë‘ ê³„íš ë³‘í•©
        hybrid_plan = self.merge_plans(rule_based_plan, ml_predicted_plan)
        
        # 4. ê²€ì¦ ë° ì¡°ì •
        validated_plan = self.validate_and_adjust(hybrid_plan)
        
        # 5. ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        validated_plan["confidence_score"] = self.calculate_confidence(
            rule_based_plan,
            ml_predicted_plan,
            validated_plan
        )
        
        return validated_plan
    
    def merge_plans(self, rule_plan, ml_plan):
        """ë‘ ê³„íš ë³‘í•©"""
        
        merged = {
            "steps": [],
            "strategy": "hybrid",
            "sources": {
                "rule_based": rule_plan,
                "ml_based": ml_plan
            }
        }
        
        # ê³µí†µ ë‹¨ê³„ ì‹ë³„
        common_steps = self.find_common_steps(rule_plan, ml_plan)
        
        # ì°¨ì´ì  ë¶„ì„
        differences = self.analyze_differences(rule_plan, ml_plan)
        
        # ìµœì  ì¡°í•© ì„ íƒ
        for step_id in range(max(len(rule_plan["steps"]), 
                                len(ml_plan["steps"]))):
            if step_id in common_steps:
                # ê³µí†µ ë‹¨ê³„ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
                merged["steps"].append(common_steps[step_id])
            else:
                # ì°¨ì´ë‚˜ëŠ” ë¶€ë¶„ì€ ì ìˆ˜ ê¸°ë°˜ ì„ íƒ
                best_step = self.select_best_step(
                    rule_plan.get("steps", []).get(step_id),
                    ml_plan.get("steps", []).get(step_id)
                )
                if best_step:
                    merged["steps"].append(best_step)
        
        return merged
```

---

## ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ê°œì„ 

### 1. ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```python
class PerformanceMonitor:
    def __init__(self):
        self.metrics = {
            "analysis_accuracy": [],
            "planning_efficiency": [],
            "execution_success_rate": [],
            "user_satisfaction": []
        }
    
    def track_analysis_performance(self, query, predicted, actual):
        """ë¶„ì„ ì„±ëŠ¥ ì¶”ì """
        
        accuracy = self.calculate_accuracy(predicted, actual)
        
        self.metrics["analysis_accuracy"].append({
            "timestamp": datetime.now(),
            "query": query,
            "accuracy": accuracy,
            "predicted_intent": predicted["intent"],
            "actual_intent": actual["intent"],
            "agent_match_rate": self.calculate_agent_match(
                predicted["required_agents"],
                actual["required_agents"]
            )
        })
        
        # ì‹¤ì‹œê°„ ì•Œë¦¼
        if accuracy < 0.7:
            self.alert_low_accuracy(query, predicted, actual)
    
    def generate_improvement_report(self):
        """ê°œì„  ë¦¬í¬íŠ¸ ìƒì„±"""
        
        report = {
            "period": "last_7_days",
            "summary": {},
            "recommendations": []
        }
        
        # í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°
        for metric_name, values in self.metrics.items():
            if values:
                report["summary"][metric_name] = {
                    "average": np.mean([v.get("accuracy", 0) for v in values]),
                    "trend": self.calculate_trend(values),
                    "issues": self.identify_issues(values)
                }
        
        # ê°œì„  ê¶Œì¥ì‚¬í•­
        report["recommendations"] = self.generate_recommendations(report["summary"])
        
        return report
```

### 2. A/B í…ŒìŠ¤íŒ… í”„ë ˆì„ì›Œí¬

```python
class ABTestingFramework:
    def run_experiment(self, query, variants):
        """A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        
        # ë³€í˜• ì„ íƒ (í™•ë¥ ì )
        selected_variant = self.select_variant(variants)
        
        # ì‹¤í–‰
        result = self.execute_variant(query, selected_variant)
        
        # ê²°ê³¼ ê¸°ë¡
        self.record_result(query, selected_variant, result)
        
        # í†µê³„ì  ìœ ì˜ì„± ê²€ì¦
        if self.has_sufficient_data():
            winner = self.determine_winner()
            if winner:
                self.promote_winner(winner)
        
        return result
    
    def create_variants(self, base_strategy):
        """ì „ëµ ë³€í˜• ìƒì„±"""
        
        variants = [
            {
                "name": "baseline",
                "strategy": base_strategy
            },
            {
                "name": "aggressive_parallel",
                "strategy": {**base_strategy, "parallel_threshold": 0.3}
            },
            {
                "name": "conservative_sequential",
                "strategy": {**base_strategy, "parallel_threshold": 0.8}
            },
            {
                "name": "cache_heavy",
                "strategy": {**base_strategy, "cache_aggressiveness": 0.9}
            }
        ]
        
        return variants
```

---

## ğŸ¯ ê²°ë¡ 

LLM ë¶„ì„ ë° ê³„íš ìˆ˜ë¦½ ë©”ì»¤ë‹ˆì¦˜ì€ Supervisor Agentì˜ í•µì‹¬ìœ¼ë¡œ, GPT-4oì˜ ê°•ë ¥í•œ ì–¸ì–´ ì´í•´ ëŠ¥ë ¥ê³¼ ì²´ê³„ì ì¸ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§, ê·¸ë¦¬ê³  ë™ì  ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ê²°í•©í•˜ì—¬ ë³µì¡í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ì„ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì§€ì†ì ì¸ í•™ìŠµê³¼ A/B í…ŒìŠ¤íŒ…ì„ í†µí•´ ì‹œìŠ¤í…œì€ ì ì§„ì ìœ¼ë¡œ ê°œì„ ë˜ë©°, ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ë”ìš± ì •í™•í•˜ê³  íš¨ìœ¨ì ì¸ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.