"""
Search Service Business Logic
í†µí•© ê²€ìƒ‰ ì„œë¹„ìŠ¤ - FAISS + BGE + GPT-4o-mini
"""
import os
import sys
import asyncio
import logging
from typing import List, Dict, Any, Optional
import time
import json
import sqlite3
from pathlib import Path

# ê³µí†µ ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'shared'))

from models import SearchResult
from openai_client import multi_gpt_client

logger = logging.getLogger(__name__)


class SearchService:
    """
    í†µí•© ê²€ìƒ‰ ì„œë¹„ìŠ¤
    - FAISS ë²¡í„° ê²€ìƒ‰
    - BGE ì¬ìˆœìœ„ ì§€ì •  
    - GPT-4o-mini í–¥ìƒ
    - í‚¤ì›Œë“œ ê²€ìƒ‰
    """
    
    def __init__(self):
        self.is_initialized = False
        self.vector_index = None
        self.embeddings_model = None
        self.bge_model = None
        self.search_stats = {
            "total_searches": 0,
            "vector_searches": 0,
            "keyword_searches": 0,
            "enhanced_searches": 0,
            "avg_response_time": 0.0,
            "last_reindex": None
        }
        
        # ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
        self.db_path = Path(__file__).parent.parent.parent.parent / "data" / "databases"
        self.documents_path = Path(__file__).parent.parent.parent.parent / "data" / "documents"
        
    async def initialize(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸ” ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. ë””ë ‰í† ë¦¬ ìƒì„±
            self.db_path.mkdir(parents=True, exist_ok=True)
            self.documents_path.mkdir(parents=True, exist_ok=True)
            
            # 2. ë²¡í„° ê²€ìƒ‰ ì´ˆê¸°í™” (ì‹œë®¬ë ˆì´ì…˜)
            await self._initialize_vector_search()
            
            # 3. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (ì‹œë®¬ë ˆì´ì…˜)
            await self._initialize_embeddings()
            
            # 4. BGE ëª¨ë¸ ì´ˆê¸°í™” (ì‹œë®¬ë ˆì´ì…˜)
            await self._initialize_bge()
            
            # 5. ìƒ˜í”Œ ë°ì´í„° ìƒì„±
            await self._create_sample_data()
            
            self.is_initialized = True
            logger.info("âœ… ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def _initialize_vector_search(self):
        """FAISS ë²¡í„° ê²€ìƒ‰ ì´ˆê¸°í™”"""
        logger.info("ğŸ“Š FAISS ë²¡í„° ì¸ë±ìŠ¤ ì´ˆê¸°í™”...")
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        self.vector_index = "faiss_index_simulation"
        await asyncio.sleep(0.1)  # ì‹œë®¬ë ˆì´ì…˜
        
    async def _initialize_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        logger.info("ğŸ”¤ KURE-v1 ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”...")
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” sentence-transformers ë¡œë“œ
        self.embeddings_model = "kure_v1_simulation"
        await asyncio.sleep(0.1)  # ì‹œë®¬ë ˆì´ì…˜
        
    async def _initialize_bge(self):
        """BGE ì¬ìˆœìœ„ ëª¨ë¸ ì´ˆê¸°í™”"""
        logger.info("ğŸ¯ BGE ì¬ìˆœìœ„ ëª¨ë¸ ì´ˆê¸°í™”...")
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” BGE ëª¨ë¸ ë¡œë“œ
        self.bge_model = "bge_simulation"
        await asyncio.sleep(0.1)  # ì‹œë®¬ë ˆì´ì…˜
    
    async def _create_sample_data(self):
        """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
        sample_documents = [
            {
                "id": "doc_001",
                "title": "ì˜ë£Œê¸°ê¸° ì˜ì—… ì „ëµ ê°€ì´ë“œ",
                "content": "ì˜ë£Œê¸°ê¸° ì˜ì—…ì—ì„œ ì¤‘ìš”í•œ ê²ƒì€ ì˜ë£Œì§„ê³¼ì˜ ì‹ ë¢° ê´€ê³„ êµ¬ì¶•ì…ë‹ˆë‹¤. ì œí’ˆì˜ ê¸°ìˆ ì  ìš°ìˆ˜ì„±ë¿ë§Œ ì•„ë‹ˆë¼ ì˜ë£Œì§„ì˜ ë‹ˆì¦ˆë¥¼ íŒŒì•…í•˜ê³  ë§ì¶¤í˜• ì†”ë£¨ì…˜ì„ ì œê³µí•˜ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤.",
                "category": "sales",
                "keywords": ["ì˜ë£Œê¸°ê¸°", "ì˜ì—…", "ì‹ ë¢°ê´€ê³„", "ë§ì¶¤í˜•ì†”ë£¨ì…˜"]
            },
            {
                "id": "doc_002", 
                "title": "ë³‘ì› êµ¬ë§¤ ë‹´ë‹¹ìì™€ì˜ ì†Œí†µ ë°©ë²•",
                "content": "ë³‘ì› êµ¬ë§¤ ë‹´ë‹¹ìì™€ íš¨ê³¼ì ìœ¼ë¡œ ì†Œí†µí•˜ê¸° ìœ„í•´ì„œëŠ” ë¹„ìš© íš¨ìœ¨ì„±, í’ˆì§ˆ ë³´ì¦, ì‚¬í›„ ì„œë¹„ìŠ¤ ë“±ì„ ëª…í™•í•˜ê²Œ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤. ë°ì´í„° ê¸°ë°˜ì˜ ì„¤ë“ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.",
                "category": "communication",
                "keywords": ["ë³‘ì›", "êµ¬ë§¤ë‹´ë‹¹ì", "ë¹„ìš©íš¨ìœ¨ì„±", "í’ˆì§ˆë³´ì¦", "ë°ì´í„°ê¸°ë°˜"]
            },
            {
                "id": "doc_003",
                "title": "ì˜ë£Œì—…ê³„ ìµœì‹  ë™í–¥ ë¶„ì„",
                "content": "2024ë…„ ì˜ë£Œì—…ê³„ëŠ” ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´, AI ì§„ë‹¨, ì›ê²© ì§„ë£Œ ë“±ì˜ íŠ¸ë Œë“œê°€ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë³€í™”ì— ë§ì¶° ì˜ì—… ì „ëµë„ ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                "category": "trends",
                "keywords": ["ë””ì§€í„¸í—¬ìŠ¤ì¼€ì–´", "AIì§„ë‹¨", "ì›ê²©ì§„ë£Œ", "2024íŠ¸ë Œë“œ"]
            }
        ]
        
        # ìƒ˜í”Œ ë¬¸ì„œë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥ (ì‹¤ì œë¡œëŠ” DBì— ì €ì¥)
        self.sample_documents = sample_documents
        logger.info(f"ğŸ“„ ìƒ˜í”Œ ë¬¸ì„œ {len(sample_documents)}ê±´ ìƒì„± ì™„ë£Œ")
    
    async def search(
        self, 
        query: str, 
        filters: Optional[Dict[str, Any]] = None,
        limit: int = 10, 
        offset: int = 0
    ) -> List[SearchResult]:
        """
        í†µí•© ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ + BGE ì¬ìˆœìœ„)
        """
        try:
            start_time = time.time()
            
            # 1ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰
            vector_results = await self.vector_search(query, limit=limit*2)
            
            # 2ë‹¨ê³„: í‚¤ì›Œë“œ ê²€ìƒ‰
            keyword_results = await self.keyword_search(query, limit=limit*2)
            
            # 3ë‹¨ê³„: ê²°ê³¼ í•©ì„± ë° ì¤‘ë³µ ì œê±°
            combined_results = self._combine_results(vector_results, keyword_results)
            
            # 4ë‹¨ê³„: BGE ì¬ìˆœìœ„
            reranked_results = await self._bge_rerank(combined_results, query)
            
            # 5ë‹¨ê³„: í•„í„°ë§ ë° í˜ì´ì§•
            filtered_results = self._apply_filters(reranked_results, filters)
            paginated_results = filtered_results[offset:offset+limit]
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.search_stats["total_searches"] += 1
            self.search_stats["avg_response_time"] = (
                self.search_stats["avg_response_time"] * (self.search_stats["total_searches"] - 1) +
                (time.time() - start_time)
            ) / self.search_stats["total_searches"]
            
            return paginated_results
            
        except Exception as e:
            logger.error(f"í†µí•© ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def vector_search(
        self, 
        query: str, 
        limit: int = 10, 
        threshold: float = 0.7
    ) -> List[SearchResult]:
        """
        FAISS ë²¡í„° ê²€ìƒ‰
        """
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” queryë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ í›„ FAISS ê²€ìƒ‰
            # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜
            
            results = []
            for i, doc in enumerate(self.sample_documents[:limit]):
                # ì‹œë®¬ë ˆì´ì…˜ëœ ìœ ì‚¬ë„ ì ìˆ˜
                score = max(0.6, 1.0 - (i * 0.1))
                
                if score >= threshold:
                    results.append(SearchResult(
                        id=doc["id"],
                        title=doc["title"],
                        content=doc["content"],
                        score=score,
                        metadata={
                            "category": doc["category"],
                            "keywords": doc["keywords"],
                            "search_type": "vector"
                        },
                        source="vector_search"
                    ))
            
            self.search_stats["vector_searches"] += 1
            return results
            
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def keyword_search(
        self, 
        query: str, 
        limit: int = 10, 
        offset: int = 0
    ) -> List[SearchResult]:
        """
        í‚¤ì›Œë“œ ê¸°ë°˜ ì „ì²´ í…ìŠ¤íŠ¸ ê²€ìƒ‰
        """
        try:
            query_words = query.lower().split()
            results = []
            
            for doc in self.sample_documents:
                # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                content_lower = (doc["title"] + " " + doc["content"]).lower()
                matches = sum(1 for word in query_words if word in content_lower)
                score = matches / len(query_words) if query_words else 0
                
                if score > 0:
                    results.append(SearchResult(
                        id=doc["id"],
                        title=doc["title"],
                        content=doc["content"],
                        score=score,
                        metadata={
                            "category": doc["category"],
                            "keywords": doc["keywords"],
                            "search_type": "keyword",
                            "matches": matches
                        },
                        source="keyword_search"
                    ))
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            results.sort(key=lambda x: x.score, reverse=True)
            
            self.search_stats["keyword_searches"] += 1
            return results[offset:offset+limit]
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def _bge_rerank(
        self, 
        results: List[SearchResult], 
        query: str
    ) -> List[SearchResult]:
        """
        BGE ëª¨ë¸ì„ ì‚¬ìš©í•œ ì¬ìˆœìœ„ ì§€ì •
        """
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” BGE ëª¨ë¸ë¡œ query-document ìœ ì‚¬ë„ ì¬ê³„ì‚°
            # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ì ìˆ˜ ì¡°ì •
            
            for result in results:
                # ì‹œë®¬ë ˆì´ì…˜: í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ì ìˆ˜ ë³´ì •
                keyword_bonus = 0.1 if any(
                    keyword.lower() in query.lower() 
                    for keyword in result.metadata.get("keywords", [])
                ) else 0
                
                result.score = min(1.0, result.score + keyword_bonus)
                result.metadata["reranked"] = True
            
            # ì¬ì •ë ¬
            results.sort(key=lambda x: x.score, reverse=True)
            return results
            
        except Exception as e:
            logger.error(f"BGE ì¬ìˆœìœ„ ì‹¤íŒ¨: {e}")
            return results  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    
    def _combine_results(
        self, 
        vector_results: List[SearchResult], 
        keyword_results: List[SearchResult]
    ) -> List[SearchResult]:
        """
        ë²¡í„° ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ í•©ì„±
        """
        combined = {}
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
        for result in vector_results:
            combined[result.id] = result
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ì¤‘ë³µ ì‹œ ë†’ì€ ì ìˆ˜ ì„ íƒ)
        for result in keyword_results:
            if result.id in combined:
                if result.score > combined[result.id].score:
                    combined[result.id] = result
            else:
                combined[result.id] = result
        
        return list(combined.values())
    
    def _apply_filters(
        self, 
        results: List[SearchResult], 
        filters: Optional[Dict[str, Any]]
    ) -> List[SearchResult]:
        """
        í•„í„° ì ìš©
        """
        if not filters:
            return results
        
        filtered = []
        for result in results:
            include = True
            
            # ì¹´í…Œê³ ë¦¬ í•„í„°
            if "category" in filters:
                if result.metadata.get("category") != filters["category"]:
                    include = False
            
            # ìµœì†Œ ì ìˆ˜ í•„í„°
            if "min_score" in filters:
                if result.score < filters["min_score"]:
                    include = False
            
            if include:
                filtered.append(result)
        
        return filtered
    
    async def enhance_results(
        self, 
        results: List[SearchResult], 
        query: str, 
        limit: int = 10
    ) -> List[SearchResult]:
        """
        GPT-4o-minië¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰ ê²°ê³¼ í–¥ìƒ
        """
        try:
            enhanced_results = []
            
            for result in results[:limit]:
                # GPT-4o-minië¡œ ì»¨í…ì¸  í–¥ìƒ
                enhanced_content = await multi_gpt_client.enhance_content(
                    content=result.content,
                    context=f"ì‚¬ìš©ì ì§ˆì˜: {query}"
                )
                
                # í–¥ìƒëœ ê²°ê³¼ ìƒì„±
                enhanced_result = SearchResult(
                    id=result.id,
                    title=result.title,
                    content=enhanced_content,
                    score=result.score,
                    metadata={
                        **result.metadata,
                        "enhanced": True,
                        "original_content": result.content
                    },
                    source=f"{result.source}_enhanced"
                )
                
                enhanced_results.append(enhanced_result)
            
            self.search_stats["enhanced_searches"] += 1
            return enhanced_results
            
        except Exception as e:
            logger.error(f"ê²°ê³¼ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return results  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    
    async def get_stats(self) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ ì„œë¹„ìŠ¤ í†µê³„ ì¡°íšŒ
        """
        return {
            **self.search_stats,
            "service_status": "healthy" if self.is_initialized else "initializing",
            "available_documents": len(self.sample_documents),
            "supported_features": [
                "FAISS Vector Search",
                "BGE Reranking",
                "GPT-4o-mini Enhancement", 
                "Keyword Search",
                "Filtering",
                "Pagination"
            ]
        }
    
    async def reindex(self) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ì¬ì¸ë±ì‹±
        """
        try:
            start_time = time.time()
            
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ëª¨ë“  ë¬¸ì„œë¥¼ ë‹¤ì‹œ ì„ë² ë”©í•˜ê³  FAISS ì¸ë±ìŠ¤ ì¬ìƒì„±
            await asyncio.sleep(1)  # ì‹œë®¬ë ˆì´ì…˜
            
            processing_time = time.time() - start_time
            self.search_stats["last_reindex"] = time.time()
            
            return {
                "reindexed_documents": len(self.sample_documents),
                "processing_time": processing_time,
                "status": "completed"
            }
            
        except Exception as e:
            logger.error(f"ì¬ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
            raise
    
    async def cleanup(self):
        """ì„œë¹„ìŠ¤ ì •ë¦¬"""
        logger.info("ğŸ§¹ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì •ë¦¬ ì¤‘...")
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‘ì—…
        self.vector_index = None
        self.embeddings_model = None
        self.bge_model = None
        self.is_initialized = False 